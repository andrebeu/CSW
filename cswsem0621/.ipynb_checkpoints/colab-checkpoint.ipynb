{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43f6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NSTATES = 9\n",
    "MAX_SCH = 200\n",
    "\n",
    "class SchemaTabularBayes():\n",
    "    \"\"\" CRP prior\n",
    "    tabluar predictive distirbution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,concentration,stickiness_wi,stickiness_bt,sparsity,\n",
    "        lrate=1,lratep=1,pvar=0,decay_rate=1,schidx=None):\n",
    "        self.Tmat = np.zeros([NSTATES,NSTATES])\n",
    "        self.alfa = concentration\n",
    "        self.beta_wi = stickiness_wi\n",
    "        self.beta_bt = stickiness_bt - np.abs(pvar*np.random.randn(1)[0])\n",
    "        self.lrate = lrate # lrate like\n",
    "        self.lratep = lratep # lrate prior\n",
    "        self.lmbda = sparsity\n",
    "        self.ntimes_sampled = 0\n",
    "        self.schidx = schidx\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def get_prior(self,beta_mode,ztm1,ztrm1):\n",
    "        \"\"\" beta_mode controls whether to combine betas or use separate\n",
    "        ztm1 : z of tstep t minus 1\n",
    "        ztrm1 : z of trial t minus 1\n",
    "        \"\"\"\n",
    "        if self.ntimes_sampled == 0:\n",
    "            return self.alfa\n",
    "        ztm1_flag = ztm1 == self.schidx\n",
    "        ztrm1_flag = ztrm1 == self.schidx\n",
    "        if beta_mode == 0: # beta within\n",
    "            crp = self.lratep*self.ntimes_sampled + self.beta_wi* ztm1_flag\n",
    "        elif beta_mode == 1: # beta between\n",
    "            assert ztm1 == ztrm1\n",
    "            crp = self.lratep*self.ntimes_sampled + self.beta_bt* ztm1_flag\n",
    "        elif beta_mode == 2: # combined\n",
    "            crp = self.lratep*self.ntimes_sampled + \\\n",
    "                    self.beta_bt*ztrm1_flag + self.beta_wi*ztm1_flag\n",
    "        return crp\n",
    "\n",
    "    def get_like(self,xtm1,xt):\n",
    "        PARAM_S = 2\n",
    "        num = self.lmbda + self.Tmat[xtm1,xt]\n",
    "        den = (PARAM_S*self.lmbda) + self.Tmat[xtm1,:].sum()\n",
    "        like = num/den\n",
    "        return like\n",
    "\n",
    "    def update(self,xtm1,xt):\n",
    "        self.Tmat[xtm1,xt]+=self.lrate\n",
    "        return None\n",
    "\n",
    "    def decay(self):\n",
    "        self.Tmat = self.Tmat*self.decay_rate\n",
    "        return None\n",
    "\n",
    "    def predict(self,xtm1):\n",
    "        \"\"\" returns un-normalized count \"\"\"\n",
    "        xthat = np.array([\n",
    "            self.get_like(xtm1,x) for x in range(NSTATES)\n",
    "            ])\n",
    "        return xthat\n",
    "\n",
    "\n",
    "class SEM():\n",
    "\n",
    "    def __init__(self,schargs,beta2):\n",
    "        self.SchClass = SchemaTabularBayes\n",
    "        self.schargs = schargs\n",
    "        self.beta2_flag = beta2\n",
    "        self.init_schlib()\n",
    "\n",
    "    def init_schlib(self):\n",
    "        \"\"\" \n",
    "        initialize with two schemas\n",
    "        one active one inactive\n",
    "        \"\"\"\n",
    "        sch0 = self.SchClass(**self.schargs,schidx=0)\n",
    "        sch1 = self.SchClass(**self.schargs,schidx=1)\n",
    "        self.schlib = [sch0,sch1]\n",
    "        return None\n",
    "\n",
    "    def decay_allsch(self):\n",
    "        for sch in self.schlib:\n",
    "            sch.decay()\n",
    "        return None\n",
    "\n",
    "    def get_beta_mode(self):\n",
    "        if self.beta2_flag: # combined\n",
    "            return 2\n",
    "        else: # between\n",
    "            if self.tstep == 0:\n",
    "                return 1\n",
    "            else: # within\n",
    "                return 0 \n",
    "        assert False\n",
    "        return None\n",
    "\n",
    "    def calc_posteriors(self,xtm1,xt,ztm,ztrm,active_only=False):\n",
    "        \"\"\" loop over schema library\n",
    "        \"\"\"\n",
    "        beta_mode = self.get_beta_mode()\n",
    "        if active_only: # prediction\n",
    "            priors = [sch.get_prior(beta_mode,ztm,ztrm) for sch in self.schlib if sch.ntimes_sampled>0]\n",
    "            likes = [sch.get_like(xtm1,xt) for sch in self.schlib if sch.ntimes_sampled>0]\n",
    "        else: # sch inference\n",
    "            priors = [sch.get_prior(beta_mode,ztm,ztrm) for sch in self.schlib]\n",
    "            likes = [sch.get_like(xtm1,xt) for sch in self.schlib]\n",
    "            # record \n",
    "            self.data['priors'][self.tridx,self.tstep,:len(priors)] = priors\n",
    "            self.data['likes'][self.tridx,self.tstep,:len(likes)] = likes\n",
    "        posteriors = [p*l for p,l in zip(priors,likes)]\n",
    "        return posteriors\n",
    "\n",
    "    def select_sch(self,xtm1,xt,ztm,ztrm):\n",
    "        \"\"\" xt and xtm1 are ints\n",
    "        \"\"\"\n",
    "        posteriors = self.calc_posteriors(xtm1,xt,ztm,ztrm)\n",
    "        self.data['post'][self.tridx,self.tstep,:len(posteriors)] = posteriors\n",
    "        active_k = np.argmax(posteriors)\n",
    "        if active_k == len(self.schlib)-1:\n",
    "            self.schlib.append(self.SchClass(**self.schargs,schidx=len(self.schlib)))\n",
    "        return active_k\n",
    "\n",
    "    def predict(self,xtm1,ztm,ztrm):\n",
    "        \"\"\" \n",
    "        \"\"\"\n",
    "        pr_xt_z = np.array([\n",
    "            self.calc_posteriors(xtm1,x,ztm,ztrm,active_only=True) for x in range(NSTATES)\n",
    "            ]) # probability of each next state under each schema\n",
    "        pr_xtp1 = np.mean(pr_xt_z,axis=1) # sum over schemas\n",
    "        # print(pr_xtp1)\n",
    "        return pr_xtp1\n",
    "\n",
    "    def run_exp(self,exp):\n",
    "        \"\"\" exp is L of trialL\n",
    "        trialL is L of obs (ints) \n",
    "        \"\"\"\n",
    "        ## recording\n",
    "        Mt0ij = np.zeros([len(exp),NSTATES,NSTATES])\n",
    "        self.data = data = {\n",
    "            'zt':-np.ones([len(exp),len(exp[0])]),\n",
    "            'xth':-np.ones([len(exp),len(exp[0]),NSTATES]),\n",
    "            'priors':-np.ones([len(exp),len(exp[0]),MAX_SCH]),\n",
    "            'likes':-np.ones([len(exp),len(exp[0]),MAX_SCH]),\n",
    "            'post':-np.ones([len(exp),len(exp[0]),MAX_SCH]),\n",
    "        }\n",
    "        ## \n",
    "        scht = schtm = schtrm = self.schlib[0] # sch0 is active to start\n",
    "        scht.ntimes_sampled += 1\n",
    "        for tridx,trialL in enumerate(exp):\n",
    "            self.tridx = tridx\n",
    "            for tstep,(xtm,xt) in enumerate(zip(trialL[:-1],trialL[1:])):\n",
    "                if len(self.schlib)>=MAX_SCH:\n",
    "                    return data\n",
    "                # print('ts',tstep)\n",
    "                self.tstep = tstep\n",
    "                ## prediction: marginilize over schemas\n",
    "                xth = self.predict(xtm,schtm.schidx,schtrm.schidx)\n",
    "                ## prediction: only active schema\n",
    "                # xth = scht.predict(xtm)\n",
    "                # update infered active schema\n",
    "                zt = self.select_sch(xtm,xt,schtm.schidx,schtrm.schidx)\n",
    "                scht = self.schlib[zt]\n",
    "                ## forgetting\n",
    "                self.decay_allsch()\n",
    "                # update transition matrix\n",
    "                scht.update(xtm,xt)\n",
    "                scht.ntimes_sampled += 1\n",
    "                # update schema history\n",
    "                schtm = scht\n",
    "                data['xth'][tridx][tstep] = xth\n",
    "                data['zt'][tridx][tstep] = zt       \n",
    "            # final schema of trial\n",
    "            schtrm = scht \n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Task():\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        A1,A2,B1,B2 = self._init_paths_csw()\n",
    "        self.paths = [[A1,A2],[B1,B2]]\n",
    "        self.tsteps = len(self.paths[0][0])\n",
    "        self.exp_int = None\n",
    "        return None\n",
    "\n",
    "\n",
    "    def _init_paths_csw(self):\n",
    "        \"\"\" \n",
    "        begin -> locA -> node11, node 21, node 31, end\n",
    "        begin -> locA -> node12, node 22, node 32, end\n",
    "        begin -> locB -> node11, node 22, node 31, end\n",
    "        begin -> locB -> node12, node 21, node 32, end\n",
    "        \"\"\"\n",
    "        begin = 0 # E0\n",
    "        locA,locB = 1,2 # E1\n",
    "        node11,node12 = 3,4 # E2 \n",
    "        node21,node22 = 5,6 # E3\n",
    "        node31,node32 = 7,8 # E4\n",
    "        end = 9\n",
    "        A1 = np.array([begin,locA,\n",
    "            node11,node21,node31\n",
    "            ])\n",
    "        A2 = np.array([begin,locA,\n",
    "            node12,node22,node32\n",
    "            ])\n",
    "        B1 = np.array([begin,locB,\n",
    "            node11,node22,node31\n",
    "            ])\n",
    "        B2 = np.array([begin,locB,\n",
    "            node12,node21,node32\n",
    "            ])\n",
    "        return A1,A2,B1,B2\n",
    "\n",
    "    def _init_paths_toy(self):\n",
    "        \"\"\" \n",
    "        begin -> locA -> node11, node 21, node 31, end\n",
    "        begin -> locA -> node12, node 22, node 32, end\n",
    "        begin -> locB -> node11, node 22, node 31, end\n",
    "        begin -> locB -> node12, node 21, node 32, end\n",
    "        \"\"\"\n",
    "        locA,locB = 0,1\n",
    "        node11,node12 = 2,3\n",
    "        node21,node22 = 4,5\n",
    "        A1 = np.array([locA,\n",
    "            node11,node21\n",
    "            ])\n",
    "        A2 = np.array([locA,\n",
    "            node12,node22\n",
    "            ])\n",
    "        B1 = np.array([locB,\n",
    "            node11,node22\n",
    "            ])\n",
    "        B2 = np.array([locB,\n",
    "            node12,node21\n",
    "            ])\n",
    "        return A1,A2,B1,B2\n",
    "\n",
    "\n",
    "    def get_curriculum(self,condition,n_train,n_test):\n",
    "        \"\"\" \n",
    "        order of events\n",
    "        NB blocked: ntrain needs to be divisible by 4\n",
    "        \"\"\"\n",
    "        curriculum = []   \n",
    "        if condition == 'blocked':\n",
    "            assert n_train%4==0\n",
    "            curriculum =  \\\n",
    "                [0] * (n_train // 4) + \\\n",
    "                [1] * (n_train // 4) + \\\n",
    "                [0] * (n_train // 4) + \\\n",
    "                [1] * (n_train // 4 )\n",
    "        elif condition == 'early':\n",
    "            curriculum =  \\\n",
    "                [0] * (n_train // 4) + \\\n",
    "                [1] * (n_train // 4) + \\\n",
    "                [0, 1] * (n_train // 4)\n",
    "        elif condition == 'middle':\n",
    "            curriculum =  \\\n",
    "                [0, 1] * (n_train // 8) + \\\n",
    "                [0] * (n_train // 4) + \\\n",
    "                [1] * (n_train // 4) + \\\n",
    "                [0, 1] * (n_train // 8)\n",
    "        elif condition == 'late':\n",
    "            curriculum =  \\\n",
    "                [0, 1] * (n_train // 4) + \\\n",
    "                [0] * (n_train // 4) + \\\n",
    "                [1] * (n_train // 4)\n",
    "        elif condition == 'interleaved':\n",
    "            curriculum = [0, 1] * (n_train // 2)\n",
    "        elif condition == 'single': ## DEBUG\n",
    "            curriculum =  \\\n",
    "                [0] * (n_train) \n",
    "        else:\n",
    "            print('condition not properly specified')\n",
    "            assert False\n",
    "        # \n",
    "        curriculum += [int(np.random.rand() < 0.5) for _ in range(n_test)]\n",
    "        return np.array(curriculum)\n",
    "\n",
    "\n",
    "    def generate_experiment(self,condition,n_train,n_test):\n",
    "        \"\"\" \n",
    "        exp: arr [ntrials,tsteps]\n",
    "        curr: arr [ntrials]\n",
    "        \"\"\"\n",
    "        # get curriculum\n",
    "        n_trials = n_train+n_test\n",
    "        curr = self.get_curriculum(condition,n_train,n_test)\n",
    "        # generate trials\n",
    "        exp = -np.ones([n_trials,self.tsteps],dtype=int)\n",
    "        for trial_idx in range(n_train+n_test):\n",
    "            # select A1,A2,B1,B2\n",
    "            event_type = curr[trial_idx]\n",
    "            path_type = np.random.randint(2)\n",
    "            path_int = self.paths[event_type][path_type]\n",
    "            # embed\n",
    "            exp[trial_idx] = path_int\n",
    "        return exp,curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752ecdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from itertools import product\n",
    "import time\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "\n",
    "## import human data for fitting\n",
    "import pandas as pd\n",
    "# hdf = pd.read_csv('../human_data.csv')\n",
    "# humanB_acc,humanI_acc = hdf.loc[:,('blocked mean','interleaved mean')].values.T\n",
    "\n",
    "# from model import *\n",
    "\n",
    "def get_sm(xth,norm=True):\n",
    "  \"\"\" \n",
    "  given x_t_hat from subject\n",
    "  [trial,layer,node]\n",
    "  get 2afc normalized softmax for layer 2/3\n",
    "  return: [layer2/3,trial,node56/78]\n",
    "  norm=true \n",
    "   apply softmax to xth\n",
    "   when prediction done with multiple schemas\n",
    "  \"\"\"\n",
    "  nodes = {2:(5,6),3:(7,8)} \n",
    "  L = [] # layer 2 and 3\n",
    "  for l,ns in nodes.items():\n",
    "    y = xth[:,l,ns]\n",
    "    if norm:\n",
    "      y=softmax(y,1)\n",
    "    L.append(y)\n",
    "  return np.array(L)\n",
    "\n",
    "def get_acc(data):\n",
    "  \"\"\" \n",
    "  returns 2afc softmax of \n",
    "  layer 2/3 transitions\n",
    "  single seed\n",
    "  \"\"\"\n",
    "  ysm = get_sm(data['xth'])\n",
    "  L = []\n",
    "  for i in range(2):\n",
    "    ysml = ysm[i,:,:]\n",
    "    yt = data['exp'][:,i+3] \n",
    "    pr_yt = ysml[range(len(ysml)),yt - (5+2*i)] # \n",
    "    L.append(pr_yt)\n",
    "  return np.array(L)\n",
    "\n",
    "def unpack_acc(cbatch_data):\n",
    "    \"\"\" \n",
    "    given cbatch data (data from multiple curr and seeds)\n",
    "    return acc [curr,seed,trial]\n",
    "    \"\"\"\n",
    "    accL = [] # curr\n",
    "    for cidx in range(len(cbatch_data)):\n",
    "        acc = np.array([get_acc(sbatch) for sbatch in cbatch_data[cidx]])\n",
    "        accL.append(acc.mean(1)) # mean over layers\n",
    "    return np.array(accL)\n",
    "\n",
    "def unpack_data(cbatch_data,dtype='priors'):\n",
    "    \"\"\" unpacks batch data from multiple curr and seeds\n",
    "    dtype: priors,likes,post\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    for cidx in range(len(cbatch_data)):\n",
    "        L.append([])\n",
    "        for sbatch_data in cbatch_data[cidx]:\n",
    "            mask = np.all(sbatch_data[dtype]!=-1,0)[0]\n",
    "            L[cidx].append(sbatch_data[dtype][:,:,mask])\n",
    "    return L\n",
    "\n",
    "\n",
    "### RUN EXP\n",
    "def run_batch_exp(ns,args):\n",
    "  \"\"\" exp over seeds, \n",
    "  single task_condition / param config\n",
    "  return full data\n",
    "  \"\"\"\n",
    "  dataL = []\n",
    "  for i in range(ns):\n",
    "    task = Task()\n",
    "    sem = SEM(schargs=args['sch'],**args['sem'])\n",
    "    exp,curr  = task.generate_experiment(**args['exp'])\n",
    "    data = sem.run_exp(exp)\n",
    "    data['exp']=exp\n",
    "    dataL.append(data)\n",
    "  return dataL\n",
    "\n",
    "\n",
    "\n",
    "def run_batch_exp_curr(ns,args,currL=['blocked','interleaved']):\n",
    "  \"\"\" loop over task conditions, \n",
    "  return acc [task_condition,seed,trial]\n",
    "  \"\"\"\n",
    "  accL = []\n",
    "  dataL = []\n",
    "  # dataD = {}\n",
    "  for curr in currL:\n",
    "    args['exp']['condition'] = curr\n",
    "    ## extract other data here\n",
    "    data_batch = run_batch_exp(ns,args)\n",
    "    dataL.append(data_batch)\n",
    "    # dataD[curr] = dataL\n",
    "    ## unpack seeds and take mean over layers\n",
    "    acc = np.array([get_acc(data) for data in data_batch]).mean(1) # mean over layer\n",
    "    accL.append(acc)\n",
    "  return dataL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c025e304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## default params\n",
    "expargs = {\n",
    "  'condition':'blocked',\n",
    "  'n_train':160,\n",
    "  'n_test':40\n",
    "}\n",
    "schargs = {\n",
    "    'concentration':1.4,\n",
    "    'stickiness_wi':5000,\n",
    "    'stickiness_bt':5, # 100\n",
    "    'sparsity':0.08,\n",
    "    'pvar': 2,\n",
    "    'lrate':0.8,\n",
    "    'lratep':1,\n",
    "    'decay_rate':1,\n",
    "} \n",
    "semargs = {\n",
    "  'beta2':True\n",
    "}\n",
    "args = {\n",
    "    'sem':semargs,\n",
    "    'sch':schargs,\n",
    "    'exp':expargs\n",
    "}\n",
    "param_str = \"-\".join([\"%s_%.3f\"%(i,j) for i,j in schargs.items()])\n",
    "param_str += \"-\"+\"-\".join([\"%s_%.3f\"%(i,j) for i,j in semargs.items()])\n",
    "param_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_name = 'concentration' # \n",
    "# p_vals = np.arange(0.1,2.5,0.2)\n",
    "\n",
    "# p_name = 'stickiness_wi' # 5000\n",
    "# p_vals = np.arange(0,10000,500)\n",
    "\n",
    "# p_name = 'stickiness_bt' # 100\n",
    "# p_vals = np.arange(0,20,1)\n",
    "\n",
    "# p_name = 'sparsity' # 0.08\n",
    "# p_vals = np.arange(0.01,0.5,0.05)\n",
    "\n",
    "# p_name = 'pvar' # 0\n",
    "# p_vals = np.arange(1,2,0.1)\n",
    "\n",
    "# p_name = 'lrate' # 1\n",
    "# p_vals = np.arange(0.1,1,0.05)\n",
    "\n",
    "p_name = 'lratep' # 1\n",
    "p_vals = np.arange(0.1,1,0.05)\n",
    "\n",
    "# p_name = 'decay_rate' # 1\n",
    "# p_vals = np.arange(0.99,1.01,0.0005)\n",
    "\n",
    "p_name,p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pltsave_macc(macc,schargs=None,labL=['B','I','E','M','L'],close=True):\n",
    "  \"\"\" \n",
    "  save accuracy of individual parameter setting \n",
    "  used in paramsearch loops\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10,4))\n",
    "  ax=plt.gca()\n",
    "  for idx in range(len(macc)):\n",
    "    ax.plot(macc[idx],label=labL[idx])\n",
    "  ax.axhline(0.5,c='k')\n",
    "  plt.legend()\n",
    "  param_str = \"-\".join([\"%s_%.3f\"%(i,j) for i,j in schargs.items()])\n",
    "#   plt.savefig('figures/scratch_folders/%i/acc-%s.jpg'%(tstamp,param_str))\n",
    "  if close:\n",
    "    plt.close('all')\n",
    "  return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9db79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 15\n",
    "dfL = []\n",
    "condL = ['blocked','interleaved','early','middle','late']\n",
    "for idx,p_val in enumerate(p_vals):\n",
    "  print(idx/len(p_vals))\n",
    "  \n",
    "  args['sch'][p_name] = p_val  \n",
    "  exp_batch_data = run_batch_exp_curr(ns,args,condL)\n",
    "  ## acc\n",
    "  batch_acc = unpack_acc(exp_batch_data) # curr,seeds,trials\n",
    "  mean_acc = batch_acc.mean(1)\n",
    "  test_acc = mean_acc[:,-40:].mean(1) # curr  \n",
    "  \n",
    "  ## save traces of EML for each param setting\n",
    "#   pltsave_macc(mean_acc[2:],args['sch'],labL=['E','M','L'])\n",
    "  \n",
    "  ## record\n",
    "  gsD = {\n",
    "    **schargs,\n",
    "    **dict(zip(condL,test_acc))\n",
    "  }\n",
    "  dfL.append(gsD)\n",
    "  \n",
    "gsdf = pd.DataFrame(dfL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.gca()\n",
    "gsdf.plot(p_name,condL,ax=ax)\n",
    "ax.set_ylim(0.4,1.05)\n",
    "ax.set_ylabel('test acc')\n",
    "plt.title(param_str)\n",
    "# plt.savefig('figures/scratch/testacc-sweep_%s-default_%s-t%s.png'%(\n",
    "#   p_name,param_str,tstamp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
